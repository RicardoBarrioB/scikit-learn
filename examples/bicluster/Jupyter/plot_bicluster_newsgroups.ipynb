{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "139d6a41-164f-45e6-89ab-b0157717846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from collections import defaultdict\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans, SpectralCoclustering\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.cluster import v_measure_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef83155-e316-4160-9d78-4692eb2a93bf",
   "metadata": {},
   "source": [
    "Un biclúster es un subconjunto de filas y columnas de una matriz de datos que muestra una coherencia o similitud significativa entre sí y es útil para descubrir patrones subyacentes en conjuntos de datos multidimensionales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac00e64-1a30-4b6e-9541-0cec7e6cc62e",
   "metadata": {},
   "source": [
    "Spectral Co-clustering: busca identificar grupos coherentes tanto en las filas como en las columnas de la matriz.\n",
    "La idea principal es encontrar submatrices dentro de la matriz de datos original donde las filas y las columnas están altamente\n",
    "correlacionadas entre sí, haya patrones intrínsecos en los datos que pueden no ser evidentes mediante otros métodos de agrupamiento<p>\n",
    "Spectral: se refiere al uso de técnicas espectrales en el proceso de agrupamiento. Las técnicas espectrales involucran el análisis de las propiedades de los eigenvalores y eigenvectores de una matriz, en este caso, la matriz de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339ed617-dfb4-4abf-b1ff-69710287a5fa",
   "metadata": {},
   "source": [
    "La tokenización es el proceso de dividir una cadena de texto en unidades más pequeñas llamadas tokens. \n",
    "Estos tokens pueden ser palabras individuales, caracteres, o incluso frases completas,\n",
    "dependiendo del nivel de granularidad deseado.\n",
    "- Creamos el tokenizador y las categorias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7067d90b-a0d6-4e8b-bbde-935c2c807a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_normalizer(tokens): #Cuando exista algún token numérico, la función revisa si el primer caracter es un número y si es así lo sustituye por #NUMBER(marcador de posición)\n",
    "    \"\"\"Map all numeric tokens to a placeholder.\n",
    "\n",
    "    For many applications, tokens that begin with a number are not directly\n",
    "    useful, but the fact that such a token exists can be relevant.  By applying\n",
    "    this form of dimensionality reduction, some methods may perform better.\n",
    "    \"\"\"\n",
    "    return (\"#NUMBER\" if token[0].isdigit() else token for token in tokens)\n",
    "\n",
    "class NumberNormalizingVectorizer(TfidfVectorizer): #Crea un tokenizador, convierte un doc de texto a una matriz TF-IDF\n",
    "    def build_tokenizer(self): #modifica el metodo super para personalizar el tokenizador\n",
    "        tokenize = super().build_tokenizer()\n",
    "        return lambda doc: list(number_normalizer(tokenize(doc))) #devuelve el resultado de una lambda, en la que coge un doc, lo tokeniza y normaliza con el metodoanterior\n",
    "categories = [ #define las categorias, que parece ser un grupo de noticias\n",
    "    \"alt.atheism\",\n",
    "    \"comp.graphics\",\n",
    "    \"comp.sys.ibm.pc.hardware\",\n",
    "    \"comp.sys.mac.hardware\",\n",
    "    \"comp.windows.x\",\n",
    "    \"misc.forsale\",\n",
    "    \"rec.autos\",\n",
    "    \"rec.motorcycles\",\n",
    "    \"rec.sport.baseball\",\n",
    "    \"rec.sport.hockey\",\n",
    "    \"sci.crypt\",\n",
    "    \"sci.electronics\",\n",
    "    \"sci.med\",\n",
    "    \"sci.space\",\n",
    "    \"soc.religion.christian\",\n",
    "    \"talk.politics.guns\",\n",
    "    \"talk.politics.mideast\",\n",
    "    \"talk.politics.misc\",\n",
    "    \"talk.religion.misc\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61855419-9001-4a85-8a4c-41da117d3845",
   "metadata": {},
   "source": [
    "La descomposición de valores singulares es una técnica matemática fundamental que se utiliza en muchos algoritmos de aprendizaje automático,\n",
    "incluidos algunos métodos de agrupamiento y reducción de dimensionalidad.<p>\n",
    "Definimos dos algoritmos diferentes de agrupación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2938588-3f22-41b8-af39-c91bbac6a9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups = fetch_20newsgroups(categories=categories) #cargamos los grupos de noticias con el metodo\n",
    "y_true = newsgroups.target #se asigna la matriz de las etiquetas que contiene el doc\n",
    "\n",
    "vectorizer = NumberNormalizingVectorizer(stop_words=\"english\", min_df=5) #instanciamos la clase creada anteriormente, eliminamos las palabras en ingles y vectoriza el doc\n",
    "cocluster = SpectralCoclustering( #Realizamos el co-clustering espectral\n",
    "    n_clusters=len(categories), svd_method=\"arpack\", random_state=0 #Tantos clusteres como categorias tengamos / \n",
    "                                                                    #svd: método que se utilizará para la descomposición de valores singulares\n",
    ")\n",
    "kmeans = MiniBatchKMeans( #kmeans optimizado para grandes volumenes de datos,en lugar de utilizar todo el conjunto de datos en cada iteración, utiliza pequeños lotes de datos seleccionados aleatoriamente\n",
    "    n_clusters=len(categories), batch_size=20000, random_state=0, n_init=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176cc0f3-9c89-464d-977b-bbd41b9f3a08",
   "metadata": {},
   "source": [
    "Vectorizamos y ajustamos los datos para utilizarlos en ambos algoritmos y evaluarlos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b65530d-d9f4-437f-b39e-bee61f898555",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing...\n",
      "Coclustering...\n",
      "Done in 0.54s. V-measure: 0.4415\n",
      "MiniBatchKMeans...\n",
      "Done in 0.98s. V-measure: 0.3015\n"
     ]
    }
   ],
   "source": [
    "print(\"Vectorizing...\")\n",
    "X = vectorizer.fit_transform(newsgroups.data)\n",
    "\n",
    "print(\"Coclustering...\")\n",
    "start_time = time()\n",
    "cocluster.fit(X)\n",
    "y_cocluster = cocluster.row_labels_ \n",
    "print(\n",
    "    \"Done in {:.2f}s. V-measure: {:.4f}\".format(\n",
    "        time() - start_time, v_measure_score(y_cocluster, y_true)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"MiniBatchKMeans...\")\n",
    "start_time = time()\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "print(\n",
    "    \"Done in {:.2f}s. V-measure: {:.4f}\".format(\n",
    "        time() - start_time, v_measure_score(y_kmeans, y_true)\n",
    "    )\n",
    ")\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()  #obtenemos los features\n",
    "document_names = list(newsgroups.target_names[i] for i in newsgroups.target) #obtenemos la lista de las categorias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4849464c-1880-4343-97c5-c1ffd9b8ac84",
   "metadata": {},
   "source": [
    "El corte normalizado es una métrica utilizada para evaluar la calidad de un biclúster en un problema de co-clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8d97989-7bee-4793-bc7b-83dc742ab4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bicluster_ncut(i): #calcula el corte normalizado para un biclúster específico identificado por el índice i\n",
    "    rows, cols = cocluster.get_indices(i)\n",
    "    \n",
    "    if not (np.any(rows) and np.any(cols)): #comprueba si hay elementos en las filas y columnas del biclúster. Si no hay elementos en ninguna de las filas o columnas,\n",
    "                                            #devuelve un valor máximo de punto flotante, indicando que el biclúster no es válido  \n",
    "        import sys\n",
    "\n",
    "        return sys.float_info.max\n",
    "\n",
    "    #obtienen los índices complementarios al biclúster i:\n",
    "    row_complement = np.nonzero(np.logical_not(cocluster.rows_[i]))[0]\n",
    "    col_complement = np.nonzero(np.logical_not(cocluster.columns_[i]))[0]\n",
    "    \n",
    "    # Note: the following is identical to X[rows[:, np.newaxis],\n",
    "    # cols].sum() but much faster in scipy <= 0.16\n",
    "    weight = X[rows][:, cols].sum() #Calcula el peso del biclúster, que es la suma de los valores en las filas y columnas del biclúster\n",
    "    cut = X[row_complement][:, cols].sum() + X[rows][:, col_complement].sum() #Calcula el corte del biclúster: suma de los valores en las filas y columnas complementarias\n",
    "    \n",
    "    return cut / weight #Devuelve el corte normalizado\n",
    "\n",
    "\n",
    "def most_common(d): #ordena de forma descendente según el segundo valor del diccionario\n",
    "    \"\"\"Items of a defaultdict(int) with the highest values.\n",
    "\n",
    "    Like Counter.most_common in Python >=2.7.\n",
    "    \"\"\"\n",
    "    return sorted(d.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "\n",
    "bicluster_ncuts = list(bicluster_ncut(i) for i in range(len(newsgroups.target_names))) #Crea una lista que contiene los cortes normalizados para cada biclúster\n",
    "best_idx = np.argsort(bicluster_ncuts)[:5] #ordena los ordenar los índices de los biclústeres según sus cortes normalizados en orden ascendente y luego selecciona los primeros cinco índices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27060c05-7291-46dc-ac73-c718ff9acd4f",
   "metadata": {},
   "source": [
    "Imprimimos la información sobre los mejores biclústeres identificados en el conjunto de datos de los 20 grupos de noticias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a114ae1f-1f7a-43aa-87d8-b026edeb883a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best biclusters:\n",
      "----------------\n",
      "bicluster 0 : 8 documents, 6 words\n",
      "categories   : 100% talk.politics.mideast\n",
      "words        : cosmo, angmar, alfalfa, alphalpha, proline, benson\n",
      "\n",
      "bicluster 1 : 1948 documents, 4325 words\n",
      "categories   : 23% talk.politics.guns, 18% talk.politics.misc, 17% sci.med\n",
      "words        : gun, guns, geb, banks, gordon, clinton, pitt, cdt, surrender, veal\n",
      "\n",
      "bicluster 2 : 1259 documents, 3534 words\n",
      "categories   : 27% soc.religion.christian, 25% talk.politics.mideast, 25% alt.atheism\n",
      "words        : god, jesus, christians, kent, sin, objective, belief, christ, faith, moral\n",
      "\n",
      "bicluster 3 : 775 documents, 1623 words\n",
      "categories   : 30% comp.windows.x, 25% comp.sys.ibm.pc.hardware, 20% comp.graphics\n",
      "words        : scsi, nada, ide, vga, esdi, isa, kth, s3, vlb, bmug\n",
      "\n",
      "bicluster 4 : 2180 documents, 2802 words\n",
      "categories   : 18% comp.sys.mac.hardware, 16% sci.electronics, 16% comp.sys.ibm.pc.hardware\n",
      "words        : voltage, shipping, circuit, receiver, processing, scope, mpce, analog, kolstad, umass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"Best biclusters:\")\n",
    "print(\"----------------\")\n",
    "for idx, cluster in enumerate(best_idx):\n",
    "    n_rows, n_cols = cocluster.get_shape(cluster) #Obtiene el número de filas y columnas del biclúster\n",
    "    cluster_docs, cluster_words = cocluster.get_indices(cluster) #Obtiene los índices de los documentos y palabras que pertenecen al biclúster\n",
    "    if not len(cluster_docs) or not len(cluster_words):\n",
    "        continue\n",
    "\n",
    "    # categories\n",
    "    counter = defaultdict(int) #crea un diccionario donde el valor predeterminado de cada elemento es 0, para contar la frecuencia de las categorías de documentos dentro del biclúster\n",
    "    for i in cluster_docs:\n",
    "        counter[document_names[i]] += 1\n",
    "    cat_string = \", \".join(\n",
    "        \"{:.0f}% {}\".format(float(c) / n_rows * 100, name) #calcula el porcentaje de documentos que hay en cada categoría dentro del biclúster\n",
    "        for name, c in most_common(counter)[:3]\n",
    "    )\n",
    "\n",
    "    # words\n",
    "    out_of_cluster_docs = cocluster.row_labels_ != cluster #indica qué documentos no pertenecen al biclúster actual\n",
    "    out_of_cluster_docs = np.where(out_of_cluster_docs)[0] #obtenemos los índices de los documentos que no pertenecen al biclúster\n",
    "    word_col = X[:, cluster_words] #Selecciona las columnas relevantes del vectorizado, corresponden a las palabras presentes en el biclúster actual\n",
    "    word_scores = np.array( #calculan las puntuaciones de importancia de las palabras dentro del biclúster\n",
    "        word_col[cluster_docs, :].sum(axis=0)\n",
    "        - word_col[out_of_cluster_docs, :].sum(axis=0)\n",
    "    )\n",
    "    word_scores = word_scores.ravel()\n",
    "    important_words = list(\n",
    "        feature_names[cluster_words[i]] for i in word_scores.argsort()[:-11:-1] #Selecciona las 10 palabras más importantes dentro del biclúster\n",
    "    )\n",
    "\n",
    "    print(\"bicluster {} : {} documents, {} words\".format(idx, n_rows, n_cols))\n",
    "    print(\"categories   : {}\".format(cat_string))\n",
    "    print(\"words        : {}\\n\".format(\", \".join(important_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f620c1c-9a14-4433-a86e-5f4bfec19a42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
